package sohmmm;

class SelfOrganizingHiddenMarkovModelMap
{

   int nodesPerRow;
   int nodesPerColumn;
   double positions[][];
   HiddenMarkovModel hmm[];
   
   /*
     Constructor for a SOHMMM lattice using the appropriate number of per row
     and column nodes. All the observation symbols (case sensitive, no blanks)
     are being retrieved from the observationSymbols variable, while the
     cardinality of the state space is obtained through the stateSpaceCardinality 
     variable. Both the observation symbols and the cardinality of the state
     space are assumed to be identical among all the Hidden Markov Models of
     the SOHMMM lattice.
   */
   SelfOrganizingHiddenMarkovModelMap(int nodesPerRow, int nodesPerColumn,
                                      String observationSymbols, int stateSpaceCardinality)
   {
      this.nodesPerRow = nodesPerRow;
      this.nodesPerColumn = nodesPerColumn;
      positions = new double[nodesPerRow*nodesPerColumn][2];
      double stepY = Math.sin(Math.PI/3.0);
      double eY;
      double eX;
      hmm = new HiddenMarkovModel[nodesPerRow*nodesPerColumn];

      eY = 0.0;
      for(int iter1 = 0;iter1<nodesPerColumn;iter1++)
      {
         if(iter1%2==0)
            eX = 0.0;
         else
            eX = 0.5;
         for(int iter2 = 0;iter2<nodesPerRow;iter2++)
         {
            positions[iter1*nodesPerRow+iter2][0] = eX;
            positions[iter1*nodesPerRow+iter2][1] = eY;
            eX += 1.0;
         }
         eY += stepY;
      }

      for(int e = 0;e<hmm.length;e++)
         hmm[e] = new HiddenMarkovModel(observationSymbols, stateSpaceCardinality);
   }

   /*
     Constructor for a SOHMMM lattice based on already saved information. All
     the parameters of the SOHMMM are being retrieved from the two files and
     multiple folders which are contained in the specified folder. It is
     mandatory that all files and folders have been constructed/created with
     the use of method saveParameters().
   */
   SelfOrganizingHiddenMarkovModelMap(String folderName)
   {
      String separator = java.io.File.separator;

      String nodesPer[] = IOSequences.readSequences(folderName+separator+"nodesPer");
      nodesPerRow = Integer.parseInt(nodesPer[0]);
      nodesPerColumn = Integer.parseInt(nodesPer[1]);
      positions = IOFiles.fileToArray(folderName+separator+"positions", nodesPerRow*nodesPerColumn, 2);
      hmm = new HiddenMarkovModel[nodesPerRow*nodesPerColumn];
      for(int e = 0;e<hmm.length;e++)
         hmm[e] = new HiddenMarkovModel(folderName+separator+"HMM["+e+"]");
   }

   /*
     Saves/stores all the parameters of the SOHMMM in the specified folder
     which consists of two files and multiple folders (that contain the
     parameters of the corresponding HMMs).
   */
   void saveParameters(String folderName)
   {
      new java.io.File(folderName).mkdir();
      String separator = java.io.File.separator;

      String nodesPer[] = new String[2];
      nodesPer[0] = String.valueOf(nodesPerRow);
      nodesPer[1] = String.valueOf(nodesPerColumn);
      IOSequences.writeSequences(folderName+separator+"nodesPer", nodesPer);

      IOFiles.arrayToFile(positions, folderName+separator+"positions", false);

      for(int e = 0;e<hmm.length;e++)
         hmm[e].saveParameters(folderName+separator+"HMM["+e+"]");
   }

   /*
     Calculation of the distances between the winner node and all other nodes
     according to some type of Gaussian distribution with a spread value equal
     to sigma.

     Spread values around 0.466 result in Gaussian distances between winner
     and nearest neighbor nodes an order of magnitude different. This is due
     to the fact that the nearest neighbors' Euclidean distances are 1.
     Moreover, the following equalities apply:
       spread=0.466 => hce=0.1000092880
       spread=0.85  => hce=0.5005531348
       spread=1.32  => hce=0.7505413640
     Spread values equal to:
       Euclidean(node[0],node[node.length-1])/sqrt(-8*ln(vL))
     result in Gaussian distances between winner and half diameter neighbors
     nearly equal to vL, and also is Gaussian distances between winner and
     maximum diameter neighbors almost equal to vL^4.
   */
   double[] calculateGaussianDistances(int c, double sigma)
   {
      double gaussianDistances[] = new double[positions.length];

      for(int e = 0;e<gaussianDistances.length;e++)
      {
         double sumOfSquares = (positions[c][0]-positions[e][0])*(positions[c][0]-positions[e][0])
                               +(positions[c][1]-positions[e][1])*(positions[c][1]-positions[e][1]);
         gaussianDistances[e] = Math.exp(-sumOfSquares/(2.0*sigma*sigma));
      }
      return gaussianDistances;
   }

   /*
     Constructs a sequence of values generated by a monotonic linear (increasing
     or decreasing) function. The number of epochs must be strictly greater
     than one (epochs>1), for obvious reasons of consistency. The
     initialValue and finalValue may follow an ascending or descending order
     (or even be equal).
   */
   double[] monotonicLinear(int epochs, double initialValue, double finalValue)
   {
      double valueSequence[] = new double[epochs];

      double linearDecreaseStep = (finalValue-initialValue)/(epochs-1);
      valueSequence[0] = initialValue;
      for(int iter = 1;iter<valueSequence.length;iter++)
         valueSequence[iter] = valueSequence[iter-1]+linearDecreaseStep;
      return valueSequence;
   }

   /*
     Constructs a sequence of values generated by a monotonic inverse
     (increasing or decreasing) function. The number of epochs must be
     strictly greater than one (epochs>1). The initialValue and finalValue
     may follow an ascending or descending order (or even be equal) as long
     as they are both positive or negative.

       p
     -----  0<=y<=epochs-1  
     y + q

     The gradient of the function's curve is controlled by the initialValue
     and finalValue parameters. In case their difference is small (large) the
     gradient under consideration obtains low (high) values.
   */   
   double[] monotonicInverse(int epochs, double initialValue, double finalValue)
   {
      double valueSequence[] = new double[epochs];
      double numerator = initialValue*finalValue*(epochs-1);
      double partialDenominator = finalValue*(epochs-1);
      double factor = initialValue-finalValue;

      for(int iter = 0;iter<valueSequence.length;iter++)
      {
         valueSequence[iter] = numerator/(factor*iter+partialDenominator);
      }
      return valueSequence;
   }

   /*
     Constructs a sequence of values generated by a monotonic exponential
     (increasing or decreasing) function. The number of epochs must be
     strictly greater than one (epochs>1). The initialValue and finalValue
     may follow an ascending or descending order (or even be equal) as long
     as they are both positive or negative.

                   FACTOR*y
     COEFFICIENT*(e        ),  0<=y<=epochs-1  

     The gradient of the function's curve is controlled by the initialValue
     and finalValue parameters. In case their difference is small (large) the
     gradient under consideration obtains low (high) values.
     It must be noticed that the exponential sequence of values appears to be
     more uniformly sampled in comparison to the inverse sequence of values
     and thus should be preferred in most cases.
   */   
   double[] monotonicExponential(int epochs, double initialValue, double finalValue)
   {
      double valueSequence[] = new double[epochs];
      double factor = Math.log(finalValue/initialValue)/(double)(epochs-1);

      for(int iter = 0;iter<valueSequence.length;iter++)
      {
         valueSequence[iter] = initialValue*Math.exp(factor*iter);
      }
      return valueSequence;
   }

   /*
     Produces a rough (maximum value) estimation of the learning rate parameter,
     under the assumption that the neighbor function's value is one along
     all HMMs. Actually it ensures that the log-likelihoods are not decreasing
     when all HMMs and all observation sequences are taken into consideration.
     Since the log-likelihoods are decreasing for values smaller than the
     estimated one (as far as experiments can confirm), the winner node's
     neigbhors should also exhibit a "decreasing" attitude when the estimated
     learning rate is employed.
   */
   double estimateLearningRate(Sequences sequences[])
   {
      double previousLogLikelihood[][] = new double[sequences.length][hmm.length];
      int pointer;
      boolean probabilityDecrease;
      double learningRate[] = {1.0, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1,
                               0.09, 0.08, 0.07, 0.06, 0.05, 0.04, 0.03, 0.02, 0.01,
                               0.009, 0.008, 0.007, 0.006, 0.005, 0.004, 0.003, 0.002, 0.001,
                               0.0009, 0.0008, 0.0007, 0.0006, 0.0005, 0.0004, 0.0003, 0.0002, 0.0001
                               };
      HiddenMarkovModel replica;
      double nextLogLikelihood;

      for(int d = 0;d<sequences.length;d++)
         for(int e = 0;e<hmm.length;e++)
            previousLogLikelihood[d][e] = hmm[e].logLikelihoodComputation(sequences[d].o);
      pointer = -1;
      do
      {
         pointer++;
         probabilityDecrease = false;
         for(int d = 0;d<sequences.length&&!probabilityDecrease;d++)
         {
            for(int e = 0;e<hmm.length&&!probabilityDecrease;e++)
            {
               replica = hmm[e].replicate();
               replica.scaledOnlineGradientDescentLearning(learningRate[pointer], sequences[d].o);
               nextLogLikelihood = replica.logLikelihoodComputation(sequences[d].o);
               if(nextLogLikelihood<previousLogLikelihood[d][e])
                  probabilityDecrease = true;
            }
         }
//         System.out.println(learningRate[pointer]+"\t["+!probabilityDecrease+"]");
      }
      while(pointer<learningRate.length-1&&probabilityDecrease);

      return learningRate[pointer];
   }

   /*
     Train a SOHMMM on the given data for the desired number of epochs.
     The remaining hyperparameters and training options are defined inside
     the train() method. In certain cases i.e. initialLearningRate there is
     also the capability to have an automated estimate.     
    */
   void train(String sequencesFilename, String identifiersFilename, int epochs)
   {
      Sequences sequences[];
      double initialLearningRate;
      double finalLearningRate;
      double learningRate[];
      double standardDeviation[];
      double maximumLogLikelihood;
      double logLikelihood;
      int bestMatchingNodeIndex = -1;
      double gaussianDistances[];
      double hce;

      sequences = hmm[0].loadSequencesIdentifiers(sequencesFilename, identifiersFilename, "SOHMMM");
      HiddenMarkovModel.shuffleSequences(sequences);
//      initialLearningRate = estimateLearningRate(sequences);
//      System.out.println(initialLearningRate);
      initialLearningRate = 1.0;
      finalLearningRate = 0.005;
      learningRate = monotonicLinear(epochs, initialLearningRate, finalLearningRate);
      standardDeviation = monotonicLinear(epochs, (1+Math.max(nodesPerRow,nodesPerColumn))/4.0, 1.32);
      System.out.println();
      for(int y = 0;y<epochs;y++)
      {
         for(int d = 0;d<sequences.length;d++)
         {
            maximumLogLikelihood = Double.NEGATIVE_INFINITY;
            for(int e = 0;e<hmm.length;e++)
            {
               logLikelihood = hmm[e].logLikelihoodComputation(sequences[d].o);
               if(logLikelihood>maximumLogLikelihood)
               {
                  maximumLogLikelihood = logLikelihood;
                  bestMatchingNodeIndex = e;
               }                  
            }
            gaussianDistances = calculateGaussianDistances(bestMatchingNodeIndex, standardDeviation[y]);
            for(int e = 0;e<hmm.length;e++)
            {
               hce = gaussianDistances[e]*learningRate[y];
               hmm[e].scaledOnlineGradientDescentLearning(hce, sequences[d].o);
            }
         }
         System.out.println("Epoch>-> "+(y+1)+"/"+epochs);
      }
      System.out.println();
   }

   /*
     Train a SOHMMM on the given data for the desired number of steps.
     The remaining hyperparameters and training options are defined inside
     the train() method. In certain cases i.e. initialLearningRate there is
     also the capability to have an automated estimate.     
   */
   void stepTrain(Sequences sequences[], int steps,
                  double initialLearningRate, double finalLearningRate)
   {
      double learningRate[];                        
      double sigma;
      double standardDeviation[];                   
      int d;
      double maximumLogLikelihood;
      double logLikelihood;                         
      int bestMatchingNodeIndex = -1;                    
      double gaussianDistances[];                   
      double hce;                                   

      HiddenMarkovModel.shuffleSequences(sequences);
      learningRate = monotonicExponential(steps, initialLearningRate, finalLearningRate);
      sigma = Math.hypot(positions[0][0]-positions[positions.length-1][0],
                         positions[0][1]-positions[positions.length-1][1]);
      sigma = sigma/Math.sqrt(-8.0*Math.log(0.1));
      standardDeviation = monotonicInverse(steps, sigma, 1.0);

      System.out.println();
      for(int y = 0;y<steps;y++)
      {
         d = y%sequences.length;
         maximumLogLikelihood = Double.NEGATIVE_INFINITY;
         for(int e = 0;e<hmm.length;e++)
         {
            logLikelihood = hmm[e].logLikelihoodComputation(sequences[d].o);
            if(logLikelihood>maximumLogLikelihood)
            {
               maximumLogLikelihood = logLikelihood;
               bestMatchingNodeIndex = e;
            }                  
         }
         gaussianDistances = calculateGaussianDistances(bestMatchingNodeIndex, standardDeviation[y]);
         for(int e = 0;e<hmm.length;e++)
         {
            hce = gaussianDistances[e]*learningRate[y];
            hmm[e].scaledOnlineGradientDescentLearning(hce, sequences[d].o);
         }
         if (d == sequences.length - 1)
            System.out.println("Step>-> "+(y+1)+"/"+steps);
      }
      System.out.println();
   }

   /*
     Finds the best matching node (index) among all Hidden Markov Model nodes
     with respect to the observation sequence under consideration.
   */
   int bestMatchingNode(int o[])
   {
      double logLikelihood;
      double maximumLogLikelihood = Double.NEGATIVE_INFINITY;
      int bestMatchingNodeIndex = -1;

      for(int e = 0;e<hmm.length;e++)
      {
         logLikelihood = hmm[e].logLikelihoodComputation(o);
         if(logLikelihood>maximumLogLikelihood)
         {
            maximumLogLikelihood = logLikelihood;
            bestMatchingNodeIndex = e;
         }
      }

      return bestMatchingNodeIndex;
   }

   /*
     Returns a matrix containing the observation sequences' names assigned to
     each individual Hidden Markov Model node. Each entry consists of the
     sequences' identifiers delimited by &&, trailing && (empty strings)
     are ignored.
   */
   String[] detectAssignedSequences(Sequences sequences[])
   {
      String assignedSequences[] = new String[hmm.length];
      int bestMatchingNodeIndex;

      for(int e = 0;e<assignedSequences.length;e++)
         assignedSequences[e] = "";
      for(int d = 0;d<sequences.length;d++)
      {
         bestMatchingNodeIndex = bestMatchingNode(sequences[d].o);
         assignedSequences[bestMatchingNodeIndex] += sequences[d].id+"&&";
      }
      return assignedSequences;
   }
                                
   /*
     Returns a matrix containing the observation sequences' indexes which
     are assigned to each individual Hidden Markov Model node. Each entry
     consists of sequences' indexes delimited by &&, and also, ends with &&.
     Moreover, a node that is not associated to any observation sequence has
     not got an entry (actually the NULL pointer is used).
   */
   String[] detectIndexesOfAssignedSequences(Sequences sequences[])
   {
      String indexesOfAssignedSequences[] = new String[hmm.length];
      int bestMatchingNodeIndex;

      for(int e = 0;e<indexesOfAssignedSequences.length;e++)
         indexesOfAssignedSequences[e] = "";
      for(int d = 0;d<sequences.length;d++)
      {
         bestMatchingNodeIndex = bestMatchingNode(sequences[d].o);
         indexesOfAssignedSequences[bestMatchingNodeIndex] += d+"&&";
      }
      for(int e = 0;e<indexesOfAssignedSequences.length;e++)
         if(indexesOfAssignedSequences[e].equals(""))
            indexesOfAssignedSequences[e] = null;

      return indexesOfAssignedSequences;
   }

   public static void main(String args[])
   {
	  SelfOrganizingHiddenMarkovModelMap sohmmm;
	  
	  int perRow = 6;
      int perColumn = 8;
      String symbols = "QWERTYIPLKHGFDSACVNM";
      int states = 9;

      int epochs = 5;
      
      sohmmm = new SelfOrganizingHiddenMarkovModelMap(perRow, perColumn,
         symbols, states);
      sohmmm.train("Globins(seqs).txt", "Globins(ids).txt", epochs);
      
//      sohmmm = new SelfOrganizingHiddenMarkovModelMap(perRow, perColumn,
//    	         symbols, states);
//      Sequences proteins[] = sohmmm.hmm[0].loadSequencesIdentifiers(
//         "Globins(seqs).txt","Globins(ids).txt", "globin");
//      sohmmm.stepTrain(proteins, epochs*proteins.length, 0.5, 0.001);
//      String output[] = sohmmm.detectIndexesOfAssignedSequences(proteins);
//      for (int i = 0; i < output.length; i++)
//    	  System.out.println((i + 1) + "->> " + output[i]);
   }

}
